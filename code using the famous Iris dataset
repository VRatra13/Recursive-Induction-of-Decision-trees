import numpy as np

class TreeNode:
    def __init__(self, data_class=None, split_feature=None, split_threshold=None, left=None, right=None):
        self.data_class = data_class  # Class label if it's a leaf node, None otherwise
        self.split_feature = split_feature  # Feature index to split on if it's an internal node, None otherwise
        self.split_threshold = split_threshold  # Threshold for the split feature if it's an internal node, None otherwise
        self.left = left  # Left subtree
        self.right = right  # Right subtree

def entropy(labels):
    unique_labels, counts = np.unique(labels, return_counts=True)
    probabilities = counts / len(labels)
    return -np.sum(probabilities * np.log2(probabilities))

def decision_tree(X, y, max_depth=None, min_samples_split=2):
    if max_depth is not None and max_depth <= 0:
        return TreeNode(data_class=np.argmax(np.bincount(y)))

    if len(np.unique(y)) == 1:
        return TreeNode(data_class=y[0])

    if len(y) < min_samples_split:
        return TreeNode(data_class=np.argmax(np.bincount(y)))

    num_features = X.shape[1]
    best_split_feature = None
    best_split_threshold = None
    best_info_gain = -1

    base_entropy = entropy(y)

    for feature in range(num_features):
        thresholds = np.unique(X[:, feature])
        for threshold in thresholds:
            left_indices = X[:, feature] <= threshold
            right_indices = X[:, feature] > threshold

            left_entropy = entropy(y[left_indices])
            right_entropy = entropy(y[right_indices])

            info_gain = base_entropy - (len(y[left_indices]) / len(y) * left_entropy +
                                        len(y[right_indices]) / len(y) * right_entropy)

            if info_gain > best_info_gain:
                best_info_gain = info_gain
                best_split_feature = feature
                best_split_threshold = threshold

    if best_info_gain == 0:
        return TreeNode(data_class=np.argmax(np.bincount(y)))

    left_indices = X[:, best_split_feature] <= best_split_threshold
    right_indices = X[:, best_split_feature] > best_split_threshold

    left_tree = decision_tree(X[left_indices], y[left_indices], max_depth=max_depth - 1 if max_depth is not None else None, min_samples_split=min_samples_split)
    right_tree = decision_tree(X[right_indices], y[right_indices], max_depth=max_depth - 1 if max_depth is not None else None, min_samples_split=min_samples_split)

    return TreeNode(split_feature=best_split_feature, split_threshold=best_split_threshold, left=left_tree, right=right_tree)

# Example usage with the Iris dataset
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X, y = iris.data, iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

tree = decision_tree(X_train, y_train, max_depth=3, min_samples_split=5)

# Print the decision tree structure (for visualization)
def print_tree(node, depth=0):
    if node is None:
        return
    indent = "  " * depth
    if node.data_class is not None:
        print(indent + "Class:", node.data_class)
    else:
        print(indent + "Split Feature:", iris.feature_names[node.split_feature])
        print(indent + "Split Threshold:", node.split_threshold)
        print_tree(node.left, depth + 1)
        print_tree(node.right, depth + 1)

print_tree(tree)
